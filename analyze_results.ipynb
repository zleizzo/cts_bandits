{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib as plt\n",
    "from copy import deepcopy\n",
    "from experiment_with_t import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "bdry_samples = []\n",
    "n = 1000\n",
    "for i in range(n):\n",
    "    samples.append(sample(K))\n",
    "    bdry_samples.append(sample(K, t=1.))\n",
    "\n",
    "\n",
    "best_loss = float('inf')\n",
    "for model_path in os.listdir('models'):\n",
    "    PATH = f'models/{model_path}'\n",
    "    model = model_path.split(',')\n",
    "    if model[-1][-3:] != '.pt':\n",
    "        continue\n",
    "    else:\n",
    "        hl_size, reg, lr, batch_size, iters = model\n",
    "        hl_size = int(hl_size)\n",
    "        reg = float(reg)\n",
    "        lr = float(lr)\n",
    "        batch_size = int(batch_size)\n",
    "        iters = int(iters[:-3])\n",
    "\n",
    "        net = BigNet(K, hl_size)\n",
    "        net.load_state_dict(torch.load(PATH))\n",
    "\n",
    "        loss = 0.\n",
    "        bdry_loss = 0.\n",
    "        for state, bdry_state in zip(samples, bdry_samples):\n",
    "            t, s, q = state\n",
    "            _, bdry_s, bdry_q = bdry_state\n",
    "            loss += hjb_term(net, t, s, q, lam, p, K)\n",
    "            bdry_loss += hjb_bdry(net, s, q, p)\n",
    "        \n",
    "        total_loss = loss + bdry_loss\n",
    "        if total_loss < best_loss:\n",
    "            best_dict = {\n",
    "                'net':net,\n",
    "                'hl_size':hl_size,\n",
    "                'reg':reg,\n",
    "                'lr':lr,\n",
    "                'batch_size':batch_size,\n",
    "                'iters':iters,\n",
    "                'loss':loss/n,\n",
    "                'bdry_loss':bdry_loss/n,\n",
    "                'total_loss':total_loss/n\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'net': BigNet(\n",
       "   (fc1): Linear(in_features=9, out_features=30, bias=True)\n",
       "   (fc2): Linear(in_features=30, out_features=30, bias=True)\n",
       "   (fc3): Linear(in_features=30, out_features=1, bias=True)\n",
       " ),\n",
       " 'hl_size': 30,\n",
       " 'reg': 0.5,\n",
       " 'lr': 0.1,\n",
       " 'batch_size': 1,\n",
       " 'iters': 10,\n",
       " 'loss': tensor(1.2590, grad_fn=<DivBackward0>),\n",
       " 'bdry_loss': tensor([0.0023], grad_fn=<DivBackward0>),\n",
       " 'total_loss': tensor([1.2613], grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1848, 0.2888, 0.2753, 0.2511], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from experiment_with_t import policy\n",
    "t, s, q = sample(K)\n",
    "policy(net, t, s, q, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
